% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_U.R
\name{one_fit}
\alias{one_fit}
\title{Performs fit step for a single SNP.}
\usage{
one_fit(x, w, V, option, formerU, r.v)
}
\arguments{
\item{x}{unscaled coefficients for single SNP across M studies}

\item{w}{Standard errors associated with row of x SNPs}

\item{V}{Fixed factor matrix from previous iteration}

\item{option}{List of setting options (here, function accesses: traitSpecificVar, gls, carry_coeffs,regression_method,actively_calibrating_sparsity)}

\item{formerU}{Matrix of U from previous iteration to use in this one}

\item{r.v}{List of residual variances- for use only when accounting for study-specific variance}
}
\value{
a list containing: "l", the current learned row of U (k x 1) of weights for a given SNP, and the maximum possible sparsity for that run
}
\description{
Internal regression step for one SNP when learning U
Key options here include:
}
\details{
carry_coeffs: this uses the previous iteration to initialize the current one. Thought it might provide a speed boost, but the difference seemed to be minimal.
glmnet: This uses the glmnet function; just exploring options
fastReg: This one was supposed to be faster. I don't know that it was at all; but not sure about serious results on it
ridge_L: this employs L2 as opposed to L1 regression  on L
fixed_ubiq: this allows for a ubiquitous factor by removing the L1 constraint from the first column.
}
\examples{
source('../simulation/Generate_input.R');
data = generate_input(tau = tau);
F = data[[1]];
X = data[[3]];
W = data[[4]];
  option = list()
   option[['lambda1']] = 0.1;
  L_predict = fit_U(X, W, F, option);

}
